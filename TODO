[x] decide on a second library which would be simple to test on
[x] define how to access my datasets
[x] create basic testing functions for pandas and second library
[x] enable output of results
[x] fazer um commit anterior formatando o código
[x] testar execução no PCAD
[x] avaliar como obtém as métricas (memory, ram)
    tracemalloc parece mais confiável após arrumar, tenho referência. psutil mede memória virtual usada antes/depois, não parece confiável
[x] determinar quais métodos alterar/remover/adicionar
    removido: to_datetime
[x] entender problema p executar no PCAD
[x] aprofundar nas libs
[x] adicionar spark e vaex
[] começar a analisar resultados, criar visualizações
    [x] drop_duplicates pro vaex: https://github.com/vaexio/vaex/pull/1623 -> dropado
    [x] resultados do koalas estão surpreendentemente ruins, pra tudo -> a princípio descartei koalas
    [x] calc_column para pandas e modin, avaliar implementação -> parecem ok
    [x] modin estranho -> correção da engine
    [x] polars estranho -> correção método edit q estava levantando warning
    [x] modificar registro de resultados para ter ids de cada run, o script slurm -> separei resultados por máquina
    [x] to_csv distorce resultados por levar muito tempo pra todos -> usar parquet no output
    [x] resolver warning de dtypes modin -> melhoraria resultados?
    [] colocar makespan no lado direito da barra do gantt
    [] medir variância das execuções e, após garantir que ela esteja controlada, fazer gráficos a partir da run mais representativa (média)
    - sinfo | grep idle
    - salloc -p tupi -w tupi3 -t 24:00:00
    - ssh tupi3
    - https://docs.google.com/document/d/1mTrDSVC4sMwGi5ZJKdUSPPg85OACjTiEYJ2CiO9lSrc/edit#heading=h.bejgxfx3orf7

[] explorar uso de diversas máquinas
[] criar exemplos de pipelines com combinações de diferentes operações e compará-las entre as libs? 

